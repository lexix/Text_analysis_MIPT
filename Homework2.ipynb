{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2\n",
    "Гончаров Алексей, 274 группа, ФУПМ, 5 курс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные и посмотрим на сообщения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNCLASSIFIED\\nU.S. Department of State\\nCase No. F-2015-04841\\nDoc No. C05739545\\nDate: 05/13/2015\\nSTATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\\nSUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER.\\nRELEASE IN FULL\\nFrom: Sullivan, Jacob J <Sullivan11@state.gov>\\nSent: Wednesday, September 12, 2012 10:16 AM\\nTo:\\nSubject: FW: Wow\\nFrom: Brose, Christian (Armed Services) (mailto:Christian_Brose@armed-servic,essenate.govi\\nSent: Wednesday, September 12, 2012 10:09 AM\\nTo: Sullivan, Jacob J\\nSubject: Wow\\nWhat a wonderful, strong and moving statement by your boss. please tell her how much Sen. McCain appreciated it. Me\\ntoo\\nUNCLASSIFIED\\nU.S. Department of State\\nCase No. F-2015-04841\\nDoc No. C05739545\\nDate: 05/13/2015\\nSTATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\\nSUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER. STATE-5CB0045247\\n\\x0c'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Emails.csv')\n",
    "text = data.RawText\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее приводится итоговый получившийся код. Для более простой проверки удалил все неудачные или сравнимые по качеству попытки.\n",
    "\n",
    "Текст обрабатывался следующим образом:\n",
    "    0. Письмо резалось по слову UNCLASSIFIED. Таким образом выкидывались все письма, стоящие в копии, или же которые пересылались как то, на что получен ответ.\n",
    "    1. Удалялись все не-буквы\n",
    "    2. Удалялись все слова, задающие шапку письма (To, From, Attachment и прочее)\n",
    "    3. Удалялись стандартные стоп-слова\n",
    "    4. Слова лемматизировались\n",
    "    5. Выделялись 2-Коллокации\n",
    "Стэмминг и другие типы лемматизации не повышали качества. Выделение n-gramm не повышало качество по сравнению со случаем применения 2-коллокаций. 3-Коллокации также не дали существенный прирост качества. Использование цифр ухудшало качество резко. Использование полных текстов также понижало качество существенно.\n",
    "\n",
    "Далее применялся стандартный CountVectorizer. Применение TfIdf существенно не меняло качество полученных кластеров.\n",
    "После этого выделялись стоп-слова, встречающиеся более чем в 10% писем. Они тоже удалялись.\n",
    "Проводилась классификация стандартным методом K-means. Выбранное число кластеров 25. При бОльшем количестве кластеров много кластеров с 1 текстом. При меньшем количестве теряется качество интерпретации. \n",
    "Другие методы кластеризации также применялись, но качество никак не менялось, иногда становилось хуже. \n",
    "\n",
    "В конце работы приводятся предположения по совершенствованию подхода, которые я не успел реализовать, так как компилляция кода происходит довольно долго."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все числа незначимы для последующей интерпретации. Оставим только символы слов. При этом удалим все слова, задающие шапку письма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cut(message):\n",
    "    row = message.split('UNCLASSIFIED')[1]#.split('UNCLASSIFIED')[0]\n",
    "    return re.sub(r'[^\\w\\s]+|[\\d]+', r'',row).strip().replace('To','').replace('B','').replace('Re','').replace('Cc','').replace('Subject','').replace('Attachments','').replace('\\n',' ').replace('Original Message','').replace('  ',' ').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "К словам из сообщения добавим 10 лучшим коллокаций из 2х слов. Также выкинем все стоп-слова из текста, соединив их нижним подчеркиванием. Применим лемматизацию к одиночным словам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_extr(message):\n",
    "    row = cut(message)\n",
    "    words = row.split(' ')\n",
    "    row = np.array(words)[np.array(words)!='']\n",
    "    row = np.array([x for x in row if x not in stopwords.words(\"english\")])\n",
    "    finder = BigramCollocationFinder.from_words(row)\n",
    "    row = lemmatizer.lemmatize(' '.join((row)))\n",
    "    return ' '.join((row,' '.join((['_'.join((x)) for x in finder.nbest(bigram_measures.pmi, 10)]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим это преобразование ко всем текстам, кроме одного пустого"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_text = []\n",
    "for i in range(len(text)):\n",
    "    if i!=1930:\n",
    "        new_text.append(feature_extr(text[i]))\n",
    "    else:\n",
    "        new_text.append('zero')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем обрабатывать полученный текст как мешок слов. Так как коллакации мы уже выделили, то биграммы не нужно учитывать. Применим CountVectorizer со стандартными настройками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cnt = CountVectorizer()\n",
    "\n",
    "new_text2 = cnt.fit_transform(new_text)\n",
    "dictionary = cnt.vocabulary_\n",
    "vocabulary = np.array(cnt.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем слова, встречающиеся более чем в 10% датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'abedin' u'abedin_huma' u'abedinhstategov' u'also' u'august' u'back'\n",
      " u'c_date' u'call' u'case' u'case_f' u'cheryl' u'could' u'date'\n",
      " u'date_release' u'december' u'department' u'department_state' u'doc'\n",
      " u'doc_c' u'f_doc' u'friday' u'full' u'fw' u'fyi' u'get' u'good'\n",
      " u'hdrclintonemailcom' u'hrodclintonemailcom' u'huma' u'im' u'jacob'\n",
      " u'know' u'last' u'like' u'may' u'meeting' u'mills' u'millscdstategov'\n",
      " u'monday' u'need' u'new' u'one' u'part' u'pm' u'president' u'release'\n",
      " u'said' u'saturday' u'secretary' u'see' u'sent' u'september' u'state'\n",
      " u'sullivan' u'sullivanjjstategov' u'sunday' u'thursday' u'time' u'today'\n",
      " u'tuesday' u'us' u'want' u'wednesday' u'well' u'work' u'would']\n"
     ]
    }
   ],
   "source": [
    "per = np.array(new_text2.todense() > 0).sum(axis=0)/float(new_text2.shape[0])\n",
    "stop_words = vocabulary[per>0.1]\n",
    "print stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем кластеризацию полученной матрицы частот слов и коллокаций. Опытным путем подобрали количество кластеров: 25 (подбирал на уменьшенном датасете). Не привожу все выкладки, так как на полном датасете работает слишком долго. Для бОльшего количества кластеров много кластеров, где присутствует 1-2 элемента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=25, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "clf = KMeans(n_clusters=25)\n",
    "clf.fit(new_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализировать датасет решил путем вывода всех наиболее часто встречающихся слов в предложении - центре кластера. Выводим слова из словаря согласно количеству их встречаемости в самом центре кластера.\n",
    "При этом не выводим те слова, которые уже встретились в других кластерах до этого. Выводим по 20 слов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "center_unic_voc = [0]*25\n",
    "all_used_text = []\n",
    "for j in range(0,25):\n",
    "    center_unic_voc[j]=[]\n",
    "    i=0\n",
    "    number = 0\n",
    "    while i<=20:\n",
    "            k = clf.cluster_centers_[j].argsort()[-number]\n",
    "            number = number + 1\n",
    "            if vocabulary[k] not in all_used_text and vocabulary[k] not in stop_words:\n",
    "                all_used_text.append(vocabulary[k])\n",
    "                center_unic_voc[j].append(vocabulary[k])\n",
    "                i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого кластера выводятся топ 5 слов, которые встречаются там чаще всего и не входят в топ слов. После этих 5 слов выводятся самые частые 20 слов. Они раскрывают тему. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 3159 abedinhstategov_sent, ap, newsmahogany, july, march\n",
      "abedinhstategov_sent, ap, newsmahogany, july, march, h_hrodclintonemailcom, full_h, says, draft, full_abedin, cheryl_millscdstategov, go, next, annemarie, slaughter, state_case, release_part, day, news, say, going\n",
      "------------------------------------------\n",
      "cluster: 1154 draftdocx, haiti, cdm, june, january\n",
      "draftdocx, haiti, cdm, june, january, october, april, november, update, nora, kennedy, daniel, patrick, iv, richard, thank, please, dec, information, talk, philip\n",
      "------------------------------------------\n",
      "cluster: 778 hillary, email, melanne, foreign, jan\n",
      "hillary, email, melanne, foreign, jan, week, hope, verveer, think, thanks, much, message, ill, best, clinton, thx, let, judith, great, nov, women\n",
      "------------------------------------------\n",
      "cluster: 725 obama, policy, american, ut, world\n",
      "obama, policy, american, ut, world, two, mr, administration, afghanistan, obamas, years, officials, year, global, even, former, military, percent, talks, including, made\n",
      "------------------------------------------\n",
      "cluster: 631 zzq, lissa, jeffrey, dont, muscatine\n",
      "zzq, lissa, jeffrey, dont, muscatine, william, way, help, urns, latest, feltman, jim, question, asked, select, sullivannstategov, palau, memo, waiver, comm, dan\n",
      "------------------------------------------\n",
      "cluster: 623 lona, valmoro, valmoroustategov, sun, sep\n",
      "lona, valmoro, valmoroustategov, sun, sep, sat, aug, mon, mtg, tomorrow, tue, wed, assistant, thu, oct, night, yes, pir, humaclintonemailcom, shuttle, dinner\n",
      "------------------------------------------\n",
      "cluster: 237 sheet, ops, list, id, pls\n",
      "sheet, ops, list, id, pls, senator, called, vote, make, ask, send, arnett, ssaudabayev, add, following, set, jake, take, working, ob, available\n",
      "------------------------------------------\n",
      "cluster: 190 calls, ok, secure, oprah, letter\n",
      "calls, ok, secure, oprah, letter, fax, mar, told, confirmed, oscar, verma, lautenberg, uters, jun, king, asking, hes, fm, jul, come, prince\n",
      "------------------------------------------\n",
      "cluster: 90 text, davutoglu, turkeyarmenia, mashabane, mubarak\n",
      "text, davutoglu, turkeyarmenia, mashabane, mubarak, trying, thru, youcall, another, try, vermarrstategov, hang, still, line, talking, anytime, word, budget, jack, sure, holbrooke\n",
      "------------------------------------------\n",
      "cluster: 69 libya, libyan, enghazi, sensitive, source\n",
      "libya, libyan, enghazi, sensitive, source, national, magariaf, qaddafi, western, council, sources, intelligence, militias, agreement, dept, forces, hrc, transitional, redactions, foia, produced\n",
      "------------------------------------------\n",
      "cluster: 57 cameron, party, labour, ries, election\n",
      "cameron, party, labour, ries, election, david, gordon, clegg, rown, ry, deal, peter, government, sbwhoeop, uk, support, minister, conservative, poll, europe, majority\n",
      "------------------------------------------\n",
      "cluster: 46 de, children, un, la, people\n",
      "de, children, un, la, people, haitian, et, relief, efforts, el, earthquake, assistance, que, country, trafficking, preval, du, le, many, countries, child\n",
      "------------------------------------------\n",
      "cluster: 38 office, secretarys, room, en, arrive\n",
      "office, secretarys, room, en, arrive, route, depart, daily, staff, conference, private, sidence, schedule, mini, house, lauren, jiloty, white, jilotylcstategov, floor, presidential\n",
      "------------------------------------------\n",
      "cluster: 31 philippe, ines, aid, reinespstategov, mchale\n",
      "philippe, ines, aid, reinespstategov, mchale, illions, pakistan, little, dollars, buy, goodwill, mchalejastategov, lew, august_pm, workers, jacobi, friday_august, security, post, forward, raj\n",
      "------------------------------------------\n",
      "cluster: 29 airport, uilding, senate, hotel, treaty\n",
      "airport, uilding, senate, hotel, treaty, andrews, laguardia, dirksen, photo, air, ilateral, th, riefing, force, via, outer, camera, spray, york, prime, preceding\n",
      "------------------------------------------\n",
      "cluster: 28 small, weekly, senior, prerief, secretaries\n",
      "small, weekly, senior, prerief, secretaries, joint, top, east, james, availaility, team, franklin, monroe, hall, wregional, camera_spray, marshall, enjamin, george, secys, amassador\n",
      "------------------------------------------\n",
      "cluster: 18 statement, issue, ravo, rava, feb\n",
      "statement, issue, ravo, rava, feb, fri, northern, ireland, press, powersharing, february, event, justice, robinson, devolved, powers, dup, policing, find, sinn, mcguinness\n",
      "------------------------------------------\n",
      "cluster: 16 democracy, georgetown, editorial, posner, iran\n",
      "democracy, georgetown, editorial, posner, iran, mutually, cox, name, posners, policies, reinforcing, century, st, exercise, commitment, sponse, fundamental, fear, lgt, response, administrations\n",
      "------------------------------------------\n",
      "cluster: 10 food, lair, cherie, confidential, qatar\n",
      "food, lair, cherie, confidential, qatar, discuss, programme, keen, participation, meet, possible, crown, phone, international, codex, issues, partnership, mosahs, head, ank, problem\n",
      "------------------------------------------\n",
      "cluster: 8 ashton, lady, friends, trip, roberta\n",
      "ashton, lady, friends, trip, roberta, morning, jacobson, mailtohdrclintonemailcom, conf, elow, wish, spoke, forestall, gratefulness, express, give, report, briefing, first, briefing_russels, read\n",
      "------------------------------------------\n",
      "cluster: 3 wolff, permanent, emergency, presentative, alejandro\n",
      "wolff, permanent, emergency, presentative, alejandro, marks, ambassador, deputy, session, usun, erica, apologies_typos, lackberry, released, arksruggles, brevity, final, typos, apologies, ny, back_yet\n",
      "------------------------------------------\n",
      "cluster: 2 israel, settlement, freeze, netanyahu, israeli\n",
      "israel, settlement, freeze, netanyahu, israeli, cabinet, proposal, palestinians, nations, veto, peace, washington, official, west, seek, palestinian, war, incentives, captured, resolutions, diplomatic\n",
      "------------------------------------------\n",
      "cluster: 1 afpdpa, public, africa, diplomacy, ureau\n",
      "afpdpa, public, africa, diplomacy, ureau, space, development, offices, organizational, plan, audiences, core, engaging, goals, united, affairs, process, media, states, af, analysis\n",
      "------------------------------------------\n",
      "cluster: 1 jessica, lana, miller, mrs, hospital\n",
      "jessica, lana, miller, mrs, hospital, daneshforouz, mccallum, dpcstaffdpcsenategov, lewis, mary, christopher, janice, robert, mike, joshua, nicole, ray, indian, driving, charvez, helgemo\n",
      "------------------------------------------\n",
      "cluster: 1 __, speech, internet, freedom, technologies\n",
      "__, speech, internet, freedom, technologies, china, early, sid, undermine, access, points, political, used, networks, rights, human, four, technology, clintons, freedoms, progress\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in np.bincount(clf.labels_).argsort()[::-1]:\n",
    "    \n",
    "    k=center_unic_voc[i]\n",
    "    print 'cluster:',np.bincount(clf.labels_)[i],', '.join((k[:5]))\n",
    "    print ', '.join((k))\n",
    "    print '------------------------------------------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ассесору предлагается прочитать вначале 5 слов, для понимания темы кластера. Затем читается текст длиной в 20 слов. Он раскрывает и уточняет тему. Затем ассессор может согласиться, что тема через эти слова выделяется достаточно конкретно, а затем решить, хорошо ли 5 слов ее описывают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также в последующих тестированиях ассессоров можно добавить одно из 5 слов другого кластера в 5 слов кластера и дать им найти лишнее слово среди пяти. Точность такого поиска и будет итоговым качество классификации. Это стоит делать, когда слова в кластерах получились достаточно чистыми. У меня такого не присутствует. Много шума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В последующей работе можно убрать стоп-слова такие, как дни недели, названия месяцев, частицы \"ок\" и прочее. Убрать бессмысленный хлам типа \"full_h\". Для этого можно писать регулярные выражения и поставить длину слов со смыслом от 4 символов. Все это поможет сделать темы более осмысленными. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока что нет идей, как сделать кластеры более равновесными. Возможно, такое может получиться при использовании особых методов классификации и параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим все биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt = CountVectorizer(ngram_range=(2,2))\n",
    "new_text3 = cnt.fit_transform(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array(new_text3.sum(axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc2 = cnt.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упорядочим по частоте встречаемости и выведем топ по частоте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=a.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98275"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department state\n",
      "us department\n",
      "case doc\n",
      "state case\n",
      "doc date\n",
      "date release\n",
      "release part\n",
      "mills cheryl\n",
      "abedin huma\n",
      "release full\n"
     ]
    }
   ],
   "source": [
    "l=0\n",
    "number = 0\n",
    "while l<=9:\n",
    "    i = b[number]\n",
    "    number = number +1\n",
    "    if '_' not in voc2[i]:\n",
    "        print voc2[i]\n",
    "        l=l+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
